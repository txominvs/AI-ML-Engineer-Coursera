{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af3d5f4-2fec-46a0-bd39-bb3dc1f97658",
   "metadata": {},
   "source": [
    "# Graded exercise: concrete data\n",
    "\n",
    "First run all the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5797bd14-fe82-42dd-821e-920af78d4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342a952-a567-4092-9850-6dee0743333e",
   "metadata": {},
   "source": [
    "Please expand and run the cells in each section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3534e-4d93-4f6f-b721-a400d8194abe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part A: a simple Neural Network with 10 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7195d-4854-4fbf-8dc2-adaa44492664",
   "metadata": {},
   "source": [
    "Load the dataset and read the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe28ceb-aa0e-4b8d-afca-573cb36e7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_dataset = pd.read_csv(\"https://cocl.us/concrete_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc47d8d-8a49-45b4-aa66-4193bb94b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable we want to predict is the strength of the concrete\n",
    "target_variable = concrete_dataset[[\"Strength\"]]\n",
    "\n",
    "# the rest of the variables will be the features used during prediction\n",
    "features = concrete_dataset[concrete_dataset.columns[concrete_dataset.columns!=\"Strength\"]]\n",
    "num_of_features = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92656f77-0f6e-40a3-8911-e54896584a0d",
   "metadata": {},
   "source": [
    "Create a regression model with 1 single layer and 10 nodes in it. The optimizer and loss functions are 'adam' and 'mean squared error'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755cd215-32ac-4ddb-b321-de0a6cffaecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(num_of_features,)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1583e7a-3851-478a-96b8-aa63859d97fd",
   "metadata": {},
   "source": [
    "Split the data and train the model 50 times. Also, save the Mean Squared Error for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ca8026-6b2d-4416-9ebb-7bfc71b20e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a while...\n",
      "Training model 1 out of 50\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 09:18:56.498221: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2023-02-17 09:18:56.507945: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593910000 Hz\n",
      "2023-02-17 09:18:56.508753: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55af8f7f4310 executing computations on platform Host. Devices:\n",
      "2023-02-17 09:18:56.508802: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2023-02-17 09:18:56.592642: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2 out of 50\n",
      "Training model 3 out of 50\n",
      "Training model 4 out of 50\n",
      "Training model 5 out of 50\n",
      "Training model 6 out of 50\n",
      "Training model 7 out of 50\n",
      "Training model 8 out of 50\n",
      "Training model 9 out of 50\n",
      "Training model 10 out of 50\n",
      "Training model 11 out of 50\n",
      "Training model 12 out of 50\n",
      "Training model 13 out of 50\n",
      "Training model 14 out of 50\n",
      "Training model 15 out of 50\n",
      "Training model 16 out of 50\n",
      "Training model 17 out of 50\n",
      "Training model 18 out of 50\n",
      "Training model 19 out of 50\n",
      "Training model 20 out of 50\n",
      "Training model 21 out of 50\n",
      "Training model 22 out of 50\n",
      "Training model 23 out of 50\n",
      "Training model 24 out of 50\n",
      "Training model 25 out of 50\n",
      "Training model 26 out of 50\n",
      "Training model 27 out of 50\n",
      "Training model 28 out of 50\n",
      "Training model 29 out of 50\n",
      "Training model 30 out of 50\n",
      "Training model 31 out of 50\n",
      "Training model 32 out of 50\n",
      "Training model 33 out of 50\n",
      "Training model 34 out of 50\n",
      "Training model 35 out of 50\n",
      "Training model 36 out of 50\n",
      "Training model 37 out of 50\n",
      "Training model 38 out of 50\n",
      "Training model 39 out of 50\n",
      "Training model 40 out of 50\n",
      "Training model 41 out of 50\n",
      "Training model 42 out of 50\n",
      "Training model 43 out of 50\n",
      "Training model 44 out of 50\n",
      "Training model 45 out of 50\n",
      "Training model 46 out of 50\n",
      "Training model 47 out of 50\n",
      "Training model 48 out of 50\n",
      "Training model 49 out of 50\n",
      "Training model 50 out of 50\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "list_of_errors = []\n",
    "\n",
    "print(\"This will take a while...\")\n",
    "\n",
    "for i in range(50): # repeat 50 times\n",
    "    print(f\"Training model {i+1} out of 50\")\n",
    "    # create the model\n",
    "    simple_model = create_simple_model()\n",
    "    # split the data randomly\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.33)\n",
    "    # train the model\n",
    "    simple_model.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "    # evaluate its accuracy by predicting on the test set\n",
    "    predictions = simple_model.predict(x_test)\n",
    "    error_of_this_model = mean_squared_error(predictions, y_test)\n",
    "    # save the error of this model in the list\n",
    "    list_of_errors.append(error_of_this_model)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2f0074-87af-4470-a9ab-09afc1f34a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This simple model has an average error of 301\n",
      "The standard deviation of the errors is 323\n"
     ]
    }
   ],
   "source": [
    "average_error = np.mean(list_of_errors)\n",
    "standard_deviation_of_error = np.std(list_of_errors)\n",
    "\n",
    "print(f\"This simple model has an average error of {round(average_error)}\")\n",
    "print(f\"The standard deviation of the errors is {round(standard_deviation_of_error)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de067ab-678c-4dd3-bbbb-82084b18bc4d",
   "metadata": {},
   "source": [
    "## Conclusion part A\n",
    "As we can see the accuracy of the Neural Network is not very good:\n",
    "- the average error is quite high\n",
    "- furthermore, the error has a large standard deviation, so it fluctuates a lot between different models\n",
    "\n",
    "All in all we can expect this to improve using data normalization and more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143721f-6fe7-4741-95eb-fe5f57a0381f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part B: normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33921347-d0fc-40bb-870f-65c21a208a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable we want to predict is the strength of the concrete\n",
    "target_variable_normalized = (target_variable-target_variable.mean())/target_variable.std()\n",
    "scaling_of_the_target = target_variable.std().values[0]\n",
    "\n",
    "# the rest of the variables will be the features used during prediction\n",
    "features_normalized = (features-features.mean())/features.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c41c13-1692-4c09-84ed-e3aa79345d55",
   "metadata": {},
   "source": [
    "Now let us train the Neural Network again with the normalized data, 50 times using 50 epochs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ff3e18-a726-4d72-b9b6-a4522c7dd4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a while...\n",
      "Training model 1 out of 50\n",
      "Training model 2 out of 50\n",
      "Training model 3 out of 50\n",
      "Training model 4 out of 50\n",
      "Training model 5 out of 50\n",
      "Training model 6 out of 50\n",
      "Training model 7 out of 50\n",
      "Training model 8 out of 50\n",
      "Training model 9 out of 50\n",
      "Training model 10 out of 50\n",
      "Training model 11 out of 50\n",
      "Training model 12 out of 50\n",
      "Training model 13 out of 50\n",
      "Training model 14 out of 50\n",
      "Training model 15 out of 50\n",
      "Training model 16 out of 50\n",
      "Training model 17 out of 50\n",
      "Training model 18 out of 50\n",
      "Training model 19 out of 50\n",
      "Training model 20 out of 50\n",
      "Training model 21 out of 50\n",
      "Training model 22 out of 50\n",
      "Training model 23 out of 50\n",
      "Training model 24 out of 50\n",
      "Training model 25 out of 50\n",
      "Training model 26 out of 50\n",
      "Training model 27 out of 50\n",
      "Training model 28 out of 50\n",
      "Training model 29 out of 50\n",
      "Training model 30 out of 50\n",
      "Training model 31 out of 50\n",
      "Training model 32 out of 50\n",
      "Training model 33 out of 50\n",
      "Training model 34 out of 50\n",
      "Training model 35 out of 50\n",
      "Training model 36 out of 50\n",
      "Training model 37 out of 50\n",
      "Training model 38 out of 50\n",
      "Training model 39 out of 50\n",
      "Training model 40 out of 50\n",
      "Training model 41 out of 50\n",
      "Training model 42 out of 50\n",
      "Training model 43 out of 50\n",
      "Training model 44 out of 50\n",
      "Training model 45 out of 50\n",
      "Training model 46 out of 50\n",
      "Training model 47 out of 50\n",
      "Training model 48 out of 50\n",
      "Training model 49 out of 50\n",
      "Training model 50 out of 50\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "list_of_errors_norm = []\n",
    "\n",
    "print(\"This will take a while...\")\n",
    "\n",
    "for i in range(50): # repeat 50 times\n",
    "    print(f\"Training model {i+1} out of 50\")\n",
    "    # create the model\n",
    "    simple_model = create_simple_model()\n",
    "    # split the data randomly\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_normalized, target_variable_normalized, test_size=0.33)\n",
    "    # train the model\n",
    "    simple_model.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "    # evaluate its accuracy by predicting on the test set\n",
    "    predictions = simple_model.predict(x_test)\n",
    "    error_of_this_model = mean_squared_error(predictions, y_test)\n",
    "    # save the error of this model in the list\n",
    "    list_of_errors_norm.append(error_of_this_model)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c72715-d274-48ae-afd5-b2f18b4b3fdd",
   "metadata": {},
   "source": [
    "Print the error for the models using Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e4375a3-529f-44d5-81fd-4a061cd843f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normalized data and re-scaling back the error to its original magnitude, we see the error has significantly decreased to 68.88\n"
     ]
    }
   ],
   "source": [
    "average_error_normalized = np.mean(list_of_errors_norm)\n",
    "standard_deviation_of_error_normalized = np.std(list_of_errors_norm)\n",
    "\n",
    "print(f\"Using normalized data and re-scaling back the error to its original magnitude, we see the error has significantly decreased to {round(average_error_normalized*scaling_of_the_target**2,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8273ed-9346-48e2-8aa1-2530c849553a",
   "metadata": {},
   "source": [
    "## Conclusion part B\n",
    "As we can see using the average error in Part B (even once it has been re-scaled back to its non-normalized value) has greatly decreased with respect to the error in Part A, thanks to using normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394ba04-c44e-4250-bfbf-78a6b7a9901c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part C: training with 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1527f8d-6466-467f-94f9-7d33226e61ee",
   "metadata": {},
   "source": [
    "Training for a longer time, 100 epochs while still using normalized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ceffd7f-1f97-4577-a838-7515a8c929ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a while...\n",
      "Training model 1 out of 50\n",
      "Training model 2 out of 50\n",
      "Training model 3 out of 50\n",
      "Training model 4 out of 50\n",
      "Training model 5 out of 50\n",
      "Training model 6 out of 50\n",
      "Training model 7 out of 50\n",
      "Training model 8 out of 50\n",
      "Training model 9 out of 50\n",
      "Training model 10 out of 50\n",
      "Training model 11 out of 50\n",
      "Training model 12 out of 50\n",
      "Training model 13 out of 50\n",
      "Training model 14 out of 50\n",
      "Training model 15 out of 50\n",
      "Training model 16 out of 50\n",
      "Training model 17 out of 50\n",
      "Training model 18 out of 50\n",
      "Training model 19 out of 50\n",
      "Training model 20 out of 50\n",
      "Training model 21 out of 50\n",
      "Training model 22 out of 50\n",
      "Training model 23 out of 50\n",
      "Training model 24 out of 50\n",
      "Training model 25 out of 50\n",
      "Training model 26 out of 50\n",
      "Training model 27 out of 50\n",
      "Training model 28 out of 50\n",
      "Training model 29 out of 50\n",
      "Training model 30 out of 50\n",
      "Training model 31 out of 50\n",
      "Training model 32 out of 50\n",
      "Training model 33 out of 50\n",
      "Training model 34 out of 50\n",
      "Training model 35 out of 50\n",
      "Training model 36 out of 50\n",
      "Training model 37 out of 50\n",
      "Training model 38 out of 50\n",
      "Training model 39 out of 50\n",
      "Training model 40 out of 50\n",
      "Training model 41 out of 50\n",
      "Training model 42 out of 50\n",
      "Training model 43 out of 50\n",
      "Training model 44 out of 50\n",
      "Training model 45 out of 50\n",
      "Training model 46 out of 50\n",
      "Training model 47 out of 50\n",
      "Training model 48 out of 50\n",
      "Training model 49 out of 50\n",
      "Training model 50 out of 50\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "list_of_errors_norm_100epochs = []\n",
    "\n",
    "print(\"This will take a while...\")\n",
    "\n",
    "for i in range(50): # repeat 50 times\n",
    "    print(f\"Training model {i+1} out of 50\")\n",
    "    # create the model\n",
    "    simple_model = create_simple_model()\n",
    "    # split the data randomly\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_normalized, target_variable_normalized, test_size=0.33)\n",
    "    # train the model\n",
    "    simple_model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "    # evaluate its accuracy by predicting on the test set\n",
    "    predictions = simple_model.predict(x_test)\n",
    "    error_of_this_model = mean_squared_error(predictions, y_test)\n",
    "    # save the error of this model in the list\n",
    "    list_of_errors_norm_100epochs.append(error_of_this_model)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e33ed6f-647c-49e6-8e4a-2cf66ca6cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using more epochs and normalized features, the re-scaled error of the model is 51.43\n"
     ]
    }
   ],
   "source": [
    "average_error_normalized_100epochs = np.mean(list_of_errors_norm_100epochs)\n",
    "standard_deviation_of_error_normalized_100epochs = np.std(list_of_errors_norm_100epochs)\n",
    "\n",
    "print(f\"Using more epochs and normalized features, the re-scaled error of the model is {round(average_error_normalized_100epochs*scaling_of_the_target**2,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8d6e4-823c-431e-a580-89a4e4addaf0",
   "metadata": {},
   "source": [
    "## Conclusion part C\n",
    "The average error of the Neural Network in part C has decreased even further with respect to the error in Part B, by training the model in more epochs.\n",
    "\n",
    "In order to capture the complexity of the data more precisely in the next part, we will try a more **complex** neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ace778-e79b-41a5-bda6-60267c9945f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part D: more complex neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239150d0-d2e2-4790-bdb6-966a4ff39592",
   "metadata": {},
   "source": [
    "The more advanced neural network we will create will have 3 hidden layers and we will also decrease the number of epochs to 50 in order to make it faster to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90efb809-6b9d-45d8-8939-ad898d584abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complex_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(num_of_features,)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ad9325-85bf-46a3-9e4b-ac3b47b5a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take a while...\n",
      "Training model 1 out of 50\n",
      "Training model 2 out of 50\n",
      "Training model 3 out of 50\n",
      "Training model 4 out of 50\n",
      "Training model 5 out of 50\n",
      "Training model 6 out of 50\n",
      "Training model 7 out of 50\n",
      "Training model 8 out of 50\n",
      "Training model 9 out of 50\n",
      "Training model 10 out of 50\n",
      "Training model 11 out of 50\n",
      "Training model 12 out of 50\n",
      "Training model 13 out of 50\n",
      "Training model 14 out of 50\n",
      "Training model 15 out of 50\n",
      "Training model 16 out of 50\n",
      "Training model 17 out of 50\n",
      "Training model 18 out of 50\n",
      "Training model 19 out of 50\n",
      "Training model 20 out of 50\n",
      "Training model 21 out of 50\n",
      "Training model 22 out of 50\n",
      "Training model 23 out of 50\n",
      "Training model 24 out of 50\n",
      "Training model 25 out of 50\n",
      "Training model 26 out of 50\n",
      "Training model 27 out of 50\n",
      "Training model 28 out of 50\n",
      "Training model 29 out of 50\n",
      "Training model 30 out of 50\n",
      "Training model 31 out of 50\n",
      "Training model 32 out of 50\n",
      "Training model 33 out of 50\n",
      "Training model 34 out of 50\n",
      "Training model 35 out of 50\n",
      "Training model 36 out of 50\n",
      "Training model 37 out of 50\n",
      "Training model 38 out of 50\n",
      "Training model 39 out of 50\n",
      "Training model 40 out of 50\n",
      "Training model 41 out of 50\n",
      "Training model 42 out of 50\n",
      "Training model 43 out of 50\n",
      "Training model 44 out of 50\n",
      "Training model 45 out of 50\n",
      "Training model 46 out of 50\n",
      "Training model 47 out of 50\n",
      "Training model 48 out of 50\n",
      "Training model 49 out of 50\n",
      "Training model 50 out of 50\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "list_of_errors_complex_model = []\n",
    "\n",
    "print(\"This will take a while...\")\n",
    "\n",
    "for i in range(50): # repeat 50 times\n",
    "    print(f\"Training model {i+1} out of 50\")\n",
    "    # create the model\n",
    "    complex_model = create_complex_model()\n",
    "    # split the data randomly\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_normalized, target_variable_normalized, test_size=0.33)\n",
    "    # train the model\n",
    "    complex_model.fit(x_train, y_train, epochs=50, verbose=0)\n",
    "    # evaluate its accuracy by predicting on the test set\n",
    "    predictions = complex_model.predict(x_test)\n",
    "    error_of_this_model = mean_squared_error(predictions, y_test)\n",
    "    # save the error of this model in the list\n",
    "    list_of_errors_complex_model.append(error_of_this_model)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c35d954-6eb5-472f-88e6-b0577e44b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a more complex network the re-scaled average error 52.63\n"
     ]
    }
   ],
   "source": [
    "average_error_complex_model = np.mean(list_of_errors_complex_model)\n",
    "standard_deviation_of_error_complex_model = np.std(list_of_errors_complex_model)\n",
    "\n",
    "print(f\"Using a more complex network the re-scaled average error {round(average_error_complex_model*scaling_of_the_target**2,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea532f15-549f-4570-96ff-537ee2797a93",
   "metadata": {},
   "source": [
    "## Conclusion part D\n",
    "\n",
    "Finally we have seen the average error of the neural network in Part D has decreased with respect to Part B, thanks to combining feature normalization and a more complex neural network.\n",
    "\n",
    "Training the error further with 100 epoch could yield even better results, but we must be careful not to fall into the overfitting region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872aaf92-14fb-431e-958e-207dbfa27d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
