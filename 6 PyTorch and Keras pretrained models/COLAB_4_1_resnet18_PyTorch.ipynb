{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abd2f3a-7930-44bc-9a5f-3646677309bd"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0b80864-134b-4e19-b978-042c3752181d"
      },
      "source": [
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e5bdfa8-6014-495e-af4c-1905a13cf122"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53741054-55b8-4d99-9976-6032dbb90087"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3c08cf-34b0-406d-8125-0593277f34bc"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "921f07a2-6a2d-4608-84f2-0cc62bf2501b"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zriayzHGSg7O"
      },
      "source": [
        "<h2 id=\"download_data\">Setup google colab</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9D3sBckSiHs",
        "outputId": "e23dc9cd-6c34-46b9-971f-e6789b18acd4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elg_J6fPTRPv"
      },
      "outputs": [],
      "source": [
        "!rm -rf gdrive/My\\ Drive/data/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ceb7083-02ed-4c9c-b5b4-7b0418388a2a"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efedf9be-c643-4d62-8158-0918061c6b8b"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f119a703-4c0b-40c8-9ca9-152a26a98210"
      },
      "outputs": [],
      "source": [
        "!wget -P gdrive/My\\ Drive/data https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3f2804b-7bc0-4a34-a8bd-0756372003da"
      },
      "outputs": [],
      "source": [
        "!unzip -oq gdrive/My\\ Drive/data/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5119dc8-afc5-460d-879a-8b774f567bd0"
      },
      "outputs": [],
      "source": [
        "!wget -P gdrive/My\\ Drive/data https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAuwiPR_T1M2"
      },
      "outputs": [],
      "source": [
        "!unzip -oq gdrive/My\\ Drive/data/Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "720b2e1a-fa06-4daf-a922-4a70777f6709"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cadbf87-12b4-4cf5-973b-d074375b21f7"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "100c4913-0f97-425c-bf42-eba819ed5f7f",
        "outputId": "33cfcb53-4198-49d6-ff7e-c452e9f3297b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x12930a39870>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62927ada-7de8-485c-a08e-cb2b038b25d6"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63oZj4Z5cNyG"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os    \n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fed9c29-48b2-4bbf-9ba9-7f6fc1c088a2"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7b81ceb-2ff9-4e71-b0ad-bcd507f91029"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8630dc80-3ee1-40a4-84d7-0427cd7101c7"
      },
      "source": [
        " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c2612bc-5ed4-4f7d-bc9d-71c6a69ce2b7",
        "outputId": "1d829afe-83bd-432b-f42f-2de4b27fba6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\".\"\n",
        "        positive=\"Positive_tensors/Positive_tensors\"\n",
        "        negative='Negative_tensors/Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "747173bb-89d3-45e8-b058-ab209f14610c"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "0618234d-d2a4-459a-aed0-20e3803a4661",
        "outputId": "94b5f2ee-df04-4aa6-b34f-21b2d63ae076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03d6186-c5e3-4594-b469-fc776d407fe5"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67c3bc6f-c9ce-4bc6-98e2-160b4c2c6be3"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cdd3ebc-0de2-4418-9316-a20a12ec7034"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293cde0f-d36f-4584-a1ff-d4fe736b9fb0"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "model = models.resnet18(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b310a4-2eb5-4627-ae5e-d0783ba838ad"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22ed14f3-ded5-47a6-b667-34e9d5bc0b95"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f23176-eca4-4e8f-9ec2-a164a5a7ef65"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "410287ff-6594-4af8-8acc-495106d31545"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f79a8c7-4e3c-48b2-8d5c-75ec66fc7b88"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWm5rS5gcNyJ",
        "outputId": "6a41b5a7-b802-48cc-e5a3-cd2ac7620769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cuda:0\n"
          ]
        }
      ],
      "source": [
        "# enable GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Select GPU (cuda) or CPU\n",
        "model.to(device)\n",
        "print(\"using\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "048fe114-92ee-4c41-aede-1e016711ffcd"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1462f12b-da03-4175-ad74-043e46166410",
        "outputId": "61e15969-b3d3-465e-afd6-d1bb78b12380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb183bcf-8cfa-4e48-93e8-af78f42e57b0"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91768582-592a-4360-b47c-1c7db7008ff8"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8455f1a9-a0af-4502-9179-0a4693cf06d8"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5263c76f-483d-42bf-9716-c526278d3fe5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14f9645-a2ff-4900-91e7-4acf3eec2427"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f006c789-b1d6-4eb9-bdc4-613265ac440e"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a965344-294c-4f35-881b-6f3b7e938149"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ffbf141-4354-429f-ba64-cf0fecf4d97e"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "278f8e4c-8cc9-477a-b291-3aedf0d0852e"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7f9e3b-f4a4-430d-92e4-2b204f4f9162"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e10db4f0-56f4-4c94-940f-133f5764ef04",
        "outputId": "69c6b164-be70-433a-e6a0-b6de0454aebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 /300 0.6997843384742737\n",
            "10 /300 0.4247884452342987\n",
            "20 /300 0.2226848602294922\n",
            "30 /300 0.12846969068050385\n",
            "40 /300 0.11835391074419022\n",
            "50 /300 0.12032327055931091\n",
            "60 /300 0.08548160642385483\n",
            "70 /300 0.07877980917692184\n",
            "80 /300 0.0740511491894722\n",
            "90 /300 0.06186678260564804\n",
            "100 /300 0.059114620089530945\n",
            "110 /300 0.03238814324140549\n",
            "120 /300 0.04201144352555275\n",
            "130 /300 0.0447172112762928\n",
            "140 /300 0.04346326366066933\n",
            "150 /300 0.05481303110718727\n",
            "160 /300 0.051743511110544205\n",
            "170 /300 0.03227635473012924\n",
            "180 /300 0.0411820225417614\n",
            "190 /300 0.03312809020280838\n",
            "200 /300 0.09239464998245239\n",
            "210 /300 0.04472992941737175\n",
            "220 /300 0.028831960633397102\n",
            "230 /300 0.020071202889084816\n",
            "240 /300 0.029975568875670433\n",
            "250 /300 0.04425300657749176\n",
            "260 /300 0.041475385427474976\n",
            "270 /300 0.01591629907488823\n",
            "280 /300 0.01705685444176197\n",
            "290 /300 0.023681513965129852\n",
            "Evaluating...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        model.train() \n",
        "        x, y = x.to(device), y.to(device) # GPU\n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "     \n",
        "        #make a prediction \n",
        "        z = model(x)\n",
        "   \n",
        "        # calculate loss \n",
        "        loss = criterion(z, y)\n",
        "    \n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        \n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.item())\n",
        "        \n",
        "        if i%10 == 0: print(i, \"/300\", loss.item())\n",
        "\n",
        "    print(\"Evaluating...\") \n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        x_test, y_test = x_test.to(device), y_test.to(device) # GPU\n",
        "       \n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        \n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "       \n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        correct += (yhat==y_test).sum().item()\n",
        "        \n",
        "        \n",
        "    accuracy=correct/N_test\n",
        "    \n",
        "print(\"Done!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "176f3003-c65d-40bc-96ad-5c9c48c99f3b"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f321eee5-544b-4659-839f-0e6ea591d09d",
        "outputId": "02ed5131-7d17-41f8-d9a9-b50c0454fb10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9943"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c7ae1d7-abbd-4e21-b0f2-9e45b967a1b6",
        "outputId": "c1255f8a-9110-4ae6-be69-c1e0f8c60342"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA4sElEQVR4nO3dd3gc5bX48e/RSquyalaXiyT3CsZYNr03G0gICQktIYTkEhJIuwmB/AI37eYmgRRIIQQIkEZLKKYYTIAABhvbcu+2JDf13rv0/v6Y2dVKXgkJa7yS9nyex493Z0azZzT2nH27GGNQSikVusKCHYBSSqng0kSglFIhThOBUkqFOE0ESikV4jQRKKVUiAsPdgDDlZKSYnJycoIdhlJKjSkbN26sMsakBto35hJBTk4OeXl5wQ5DKaXGFBE5NNA+rRpSSqkQp4lAKaVCnCYCpZQKcZoIlFIqxGkiUEqpEOdoIhCRZSKyV0TyReTOAPtvF5Et9p8dItItIklOxqSUUqovxxKBiLiAPwDLgXnAtSIyz/8YY8y9xpiTjDEnAd8D3jHG1DgVk1JKqaM5WSJYCuQbYwqNMR3AU8AVgxx/LfCkU8HsKWvgntf2UNfS4dRHKKXUmORkIpgEHPF7X2RvO4qIxADLgGcH2H+ziOSJSF5lZeVHCuZQdQsPvF1AUW3rR/p5pZQar5xMBBJg20Cr4HwMeH+gaiFjzEPGmFxjTG5qasAR0h8qPT4KgPKGto/080opNV45mQiKgCl+7ycDJQMcew0OVgsBpMVFAlDe0O7kxyil1JjjZCLYAMwUkaki4sZ62L/Y/yARSQDOAVY4GAupdiKoaNQSgVJK+XNs0jljTJeI3AasAlzAo8aYnSJyi73/QfvQK4HXjTHNTsUCEOEKI9nj1hKBUkr14+jso8aYlcDKftse7Pf+ceBxJ+PwSouPolJLBEop1UdIjSxOj4/UEoFSSvUTUokgLS5Sew0ppVQ/IZUI0uOjqGpqp7tnoF6sSikVekIqEaTFRdJjoLpJq4eUUsorpBJBbxdSTQRKKeUVUokgPioCgMa2riBHopRSo0dIJYI4XyLoDHIkSik1eoRYIrCGTWiJQCmleoVoItASgVJKeYVYItA2AqWU6i+kEoE7PIzI8DAa2zURKKWUV0glArBKBVo1pJRSvUIuEcRHhdOgVUNKKeUTcokgLipc2wiUUspPCCYCrRpSSil/IZgIwmnSEoFSSvmEZCLQqiGllOoVgolAq4aUUspfCCaCcJo7unVNAqWUsoVcIoiNtKaZ0HYCpZSyOJoIRGSZiOwVkXwRuXOAY84VkS0islNE3nEyHuidirpBq4eUUgqAcKdOLCIu4A/ARUARsEFEXjTG7PI7JhF4AFhmjDksImlOxeOlM5AqpVRfTpYIlgL5xphCY0wH8BRwRb9jrgOeM8YcBjDGVDgYDwDJsdYqZVW6XKVSSgHOJoJJwBG/90X2Nn+zgAki8raIbBSRGwKdSERuFpE8EcmrrKw8pqAyE6IAKKtvO6bzKKXUeOFkIpAA2/p31QkHFgOXAZcAd4vIrKN+yJiHjDG5xpjc1NTUYwoqLd4qEZQ1aCJQSilwsI0AqwQwxe/9ZKAkwDFVxphmoFlE3gUWAvucCioy3EWSx62JQCmlbE6WCDYAM0Vkqoi4gWuAF/sdswI4S0TCRSQGOAXY7WBMAGTER2nVkFJK2RwrERhjukTkNmAV4AIeNcbsFJFb7P0PGmN2i8hrwDagB3jEGLPDqZi8MhI0ESillJeTVUMYY1YCK/tte7Df+3uBe52Mo7/0+Ci2Hqk7nh+plFKjVsiNLAar51B1cwcvbC7WqSaUUiEvJBNBut1z6JtPb2HDwZogR6OUUsEVkolgZnqc73Vlow4sU0qFtpBMBCdnTeDNb58DQG1LR5CjUUqp4ArJRACQnRQDQHWTJgKlVGgL2UQQ7gojITqCmmZNBEqp0BayiQAg2eOmRquGlFIhLqQTQZLHTY1WDSmlQlxIJ4IJHrdWDSmlQl5IJwKtGlJKqRBPBEkeN7XNHRijo4uVUqEr5BNBV4+hoVWXrVRKha6QTwSAVg8ppUKaJgKgplmnmVBKhS5NBOjoYqVUaNNEgM43pJQKbSGdCJI91nTU1TqWQCkVwkI6EUS7XURHuHR0sVIqpIV0IgB7mgktESilQpgmAh1drJQKcY4mAhFZJiJ7RSRfRO4MsP9cEakXkS32n/9xMp5AtESglAp14U6dWERcwB+Ai4AiYIOIvGiM2dXv0NXGmMudiuPDJHnc5Fc0BevjlVIq6JwsESwF8o0xhcaYDuAp4AoHP+8jSfK4tfuoUiqkOZkIJgFH/N4X2dv6O01EtorIqyIyP9CJRORmEckTkbzKysoRDTLJ46alo5u2zu4RPa9SSo0VTiYCCbCt/zSfm4BsY8xC4HfAC4FOZIx5yBiTa4zJTU1NHdEgfaOLtZ1AKRWinEwERcAUv/eTgRL/A4wxDcaYJvv1SiBCRFIcjOkovtHFmgiUUiHKyUSwAZgpIlNFxA1cA7zof4CIZIiI2K+X2vFUOxjTUZLtRFDVpBPPKaVCk2O9howxXSJyG7AKcAGPGmN2isgt9v4HgauAr4hIF9AKXGOO8yoxaXFRAFQ0aiJQSoUmxxIB+Kp7Vvbb9qDf698Dv3cyhg+TFm/NN1SpiUApFaJCfmRxVISLuKhwKhragh2KUkoFRcgnAoD0+CjKG7REoJQKTZoIgLS4SCoatUSglApNmgjwJgItESilQpMmAqyqoYrGdo5zhyWllBoVNBEAqXGRdHT1UN/aGexQlFLquNNEAKTF61gCpVTo0kQApMdZYwnKtQupUioEaSIAspM9AByoag5yJEopdfxpIgDS4yOJiwpnf7kuUKOUCj2aCAARYWZaLPvKG4MdilJKHXeaCGwz0+JYd6CGhT96nXWFx3UCVKWUCipNBLaZ6bEA1Ld2snJ7aZCjUUqp40cTgW1aqsf3enV+VRAjUUqp40sTge20aSnccFo2Xz57GoWVzRTVtgQ7JKWUOi40Edii3S5+fMUCPn7SRAA2H64LbkBKKXWcaCLoJ9ljDS5raNPpJpRSoUETQT+eSBcALe3dQY5EKaWOD00E/cS4rdU7m9q7ghyJUkodH44mAhFZJiJ7RSRfRO4c5LglItItIlc5Gc9QuMKE6AgXLR2aCJRSocGxRCAiLuAPwHJgHnCtiMwb4LhfAKucimW4PJHhNGnVkFIqRDhZIlgK5BtjCo0xHcBTwBUBjvsa8CxQ4WAsw+KJ1BKBUip0OJkIJgFH/N4X2dt8RGQScCXwoINxDJvHHU6zthEopUKEk4lAAmzrvxbkfcAdxphB62FE5GYRyRORvMrKypGKb0CeSJc2FiulQka4g+cuAqb4vZ8MlPQ7Jhd4SkQAUoBLRaTLGPOC/0HGmIeAhwByc3MdX1jYExlOTXOH0x+jlFKjgpOJYAMwU0SmAsXANcB1/gcYY6Z6X4vI48DL/ZNAMHgiwzlco1NMKKVCg2OJwBjTJSK3YfUGcgGPGmN2isgt9v5R1S7gz+N26YAypVTIcLJEgDFmJbCy37aACcAYc6OTsQyHJ1Ibi5VSoUNHFgfgcYfT3NGFMY43RyilVNANKRGIyDdEJF4sfxaRTSJysdPBBYsnMpweAw+9W0itNhorpca5oZYIbjLGNAAXA6nAF4CfOxZVkMXaE8/97NU9/HTl7iBHo5RSzhpqIvCOCbgUeMwYs5XA4wTGBe/EcwDdPVo9pJQa34aaCDaKyOtYiWCViMQBPc6FFVyeyN5EMCHGHcRIlFLKeUPtNfRF4CSg0BjTIiJJWNVD45J3TQKA1k7tPaSUGt+GWiI4DdhrjKkTkc8CdwH1zoUVXP4lgvpWXalMKTW+DTUR/BFoEZGFwHeBQ8BfHYsqyPwbPzQRKKXGu6Emgi5jdaq/ArjfGHM/EOdcWME1OyOOuZnxJMZEaCJQSo17Q00EjSLyPeBzwCv2YjIRzoUVXDHucF79xlmcOytVE4FSatwbaiK4GmjHGk9QhrWuwL2ORTVKJERH0NCqjcVKqfFtSInAfvj/A0gQkcuBNmPMuG0j8IqPjqChrZMeHUuglBrHhjrFxGeA9cCngc8A60bDQvNOS4iOwBho1AnolFLj2FDHEXwfWGKMqQAQkVTgDeBfTgU2GsRHW80gDa2dJESP2yYRpVSIG2obQZg3Cdiqh/GzY5b34a8Nxkqp8WyoJYLXRGQV8KT9/mr6rTMwHsVHaSJQSo1/Q0oExpjbReRTwBlY460eMsY872hko0CCX9WQUkqNV0NeocwY8yzwrIOxjDqJMVYiqGnRNQmUUuPXoIlARBqBQH0nBTDGmHhHohol0uOjiI5wkV/RFOxQlFLKMYMmAmPMuJ1GYihcYcLsjDh2lzYEOxSllHKMoz1/RGSZiOwVkXwRuTPA/itEZJuIbBGRPBE508l4Poq5mfHsKmnQ9YuVUuOWY4nAno/oD8ByYB5wrYjM63fYm8BCY8xJwE3AI07F81HNmxhPQ1sXJfVtwQ5FKaUc4WSJYCmQb4wpNMZ0AE9hzV7qY4xpMr1ftT0Ebo8IqnmZVu3Y7hKtHlJKjU9OJoJJwBG/90X2tj5E5EoR2QO8glUqOIqI3GxXHeVVVlY6EuxAZmdY7eG7tJ1AKTVOOZkIAi1uf9Q3fmPM88aYOcAngJ8EOpEx5iFjTK4xJjc1NXVko/wQsZHh5CTHaIOxUmrccjIRFAFT/N5PBkoGOtgY8y4wXURSHIzpI5mbGa+JQCk1bjmZCDYAM0Vkqoi4gWuAF/0PEJEZIiL265MBN9Y8RqPK3Mx4DtW00KSzkCqlxqEhjyweLmNMl4jcBqwCXMCjxpidInKLvf9B4FPADSLSCbQCV5tR2E9zXmY8xsDesgYWZycFOxyllBpRMgqfu4PKzc01eXl5x/UzS+paOf3nbxEXGc5vr1vEebPTjuvnK6XUsRKRjcaY3ED7xv1U0iNhYmI0v7l6Ie3dPby/vyrY4Sil1IjSRDBEVy6aTHZSDIdrWoIdilJKjShNBMOQnayJQCk1/mgiGIYpdolgrLWrKKXUYDQRDEN2UgwtHd1UNen6BEqp8UMTwTBkJccAaPWQUmpc0UQwDFlJViI4oolAKTWOaCIYhskTrERwqFoTgVJq/NBEMAxRES4y4qO0akgpNa5oIhimrKQYDtc0BzsMpZQaMZoIhilLxxIopcYZTQTDlJUUQ3lDO22d3cEORSmlRoQmgmHSnkNKqfFGE8Ew6VgCpdR4o4lgmLwlAu1CqpQaLzQRDFOyx01KrJsdJfW0d3XrvENKqTFPE8EwiQiLsyfwQUE1s+96jfve2B/skJRS6phoIvgIluQkUVLfBsC/NhYFORqllDo2mgg+gtyc3nWLs5Nj+M2/9/F+vq5cppQamxxbvH48mz8xnrS4SCoa2zlc08LawmrOmJ7CGTNSgh2aUkoNm6MlAhFZJiJ7RSRfRO4MsP96Edlm/1kjIgudjGekRLjCWHPn+XzhjByKalsxBtYfqKG5vSvYoSml1LA5lghExAX8AVgOzAOuFZF5/Q47AJxjjDkR+AnwkFPxjLRwVxgTE6J97zu6e1hbUB3EiJRS6qNxskSwFMg3xhQaYzqAp4Ar/A8wxqwxxtTabz8AJjsYz4jLSIjyvXaHh7FGE4FSagxyMhFMAo74vS+ytw3ki8CrgXaIyM0ikicieZWVlSMY4rHJtBNBSqyb7KQYiut0kJlSauxxsrFYAmwLOPpKRM7DSgRnBtpvjHkIu9ooNzd31Izg8pYIpqZ4iAx3Ud7QHuSIlFJq+JxMBEXAFL/3k4GS/geJyInAI8ByY8yYqltJj49CBHKSPfQYKCjQLqRKqbHHyaqhDcBMEZkqIm7gGuBF/wNEJAt4DvicMWafg7E4IsIVxncuns01S7NIj7e6k/b0jJoCi1JKDYljJQJjTJeI3AasAlzAo8aYnSJyi73/QeB/gGTgAREB6DLG5DoVkxNuPW8GADtL6unuMVQ3d5AaFxnkqJRSaugcHVBmjFkJrOy37UG/118CvuRkDMdLWpzVXlDe0EZqXCRrC6qZ4IlgTkZ8kCNTSqnB6RQTIyQ93ioFlNlzEF378Acsu2+1LmCjlBr1NBGMEG8Poi/9NY/fvtk7I+mvXt8brJCUUmpINBGMkJTY3naBX/+7t91bVzJTSo12mghGSIQrjKgI69fpdoXZ24SKRh1boJQa3TQRjKB137uQ607JoqO7B4AFkxKoaGjXVcyUUqOaJoIRlBATQba9pjHACZMS6Ojuoa6lM4hRKaXU4DQRjLDMxN4ZSRdMSgDQ6iGl1KimiWCETbR7D8VGhpOT7AGssQVKKTVaaSIYYRPtEkFaXKRvbEH/RPDGrnJW7Sw77rEppVQgulTlCEuLiyRMIDUu0jfa2L9qqKCyiS/9NQ+AAz+7FHtqDaWUChotEYywcFcYWUkxTEmKIdrtIj4qnAq7RFDX0sGt/9jkO1bHGCilRgNNBA74841LuGPZHMAacXykthWArz25mcKqZu6+3Fqxc11hTdBiVEopL00EDpieGuubgfTUacmsKahiTX4Vq/dX8Z2LZ3HTGTkkedx8cGBMLb+glBqnNBE4bPmCTNo6e7jukXUkREdw3SnZiAinTU/m3X1VdHX3ODbgrKWjy5HzKqXGF00EDls6Ncn3+u7L5xEbabXPX3ZCJlVN7Zxz79vc+NgG3zEPv1vIFx5bf8yfu6O4nhN++DqFlU3HfC6l1PimvYYc5goTnr75VESkT1I4f04aHreL4rpWiuta6ekxiMDjaw5SWt9KR1cP7vAw3tlXyR/fzmdiYjS/vGohYWFD62VUWNVMd4+hsLKZaamxTl2eUmoc0BLBcXDKtOQ+SQAgKsLF9adm+96XN7axrajeSgoGimpbOFLTwm3/2MSeskae21TMO/sqAWvfA2/nD1qlVN/SAUBNc4cDV6SUGk80EQTR/7t0Lk986RQACiubWbm91LfvUHUL96zaS48xPPeV00mPj+TP7x0AYMWWEu55bS8l9QOPWPbOb1TToolAKTU4TQRBNjXVmoaisLKJlTtKWTDJWtpy3YEaXt1eyrVLs5iWGstncqfwXn4VTe1dvpHKxXa31EDqWu1EoCUCpdSH0EQQZBnxUcS4Xby0tZQjNa3ccGoOHreLx94/QLcxfP70HADf2seHqpupaLBGKhfXDTwgzVsiqG4aeiLYdLhWexopFYIcTQQiskxE9opIvojcGWD/HBFZKyLtIvIdJ2MZrUSEiYnRrD9YQ3iYcPH8dLKSPbR39XDh3HSm2NNaZydbfx+qbqG80SoRFNUMXCKob/W2EQxt5tPa5g6u+uManlh3+FguRyk1BjmWCETEBfwBWA7MA64VkXn9DqsBvg780qk4xoIlORMA+PoFM0mMcfvWNPjCGTm+Y7yJ4GCfEsEgVUO+NoJOiutaOfMXb7G/vHHA44tqrUbqgsrmY7qW4Who66RAu7cqFXROdh9dCuQbYwoBROQp4Apgl/cAY0wFUCEilzkYx6j3P5fP57uXzGGCxw3A8hMycIeHcdq0ZN8xcVERpMS6OVjVTGXjEBKBr42gnY2HaimqbeWdfZXMTI8LeLz3XIdrjl8i+OPbBTy94Qib7r7ouH2mUupoTlYNTQKO+L0vsrcNm4jcLCJ5IpJXWVk5IsGNJtFuly8JAFxx0iR+e+2io2YmzU72sPVIvW8pTP/G4nf3VXLp/aupt0sCvhJBU4dvUNn24voBYyitt851qPr4TYRXVt9GTXMHbZ3dx+0zlVJHczIRBBr59JHmUjDGPGSMyTXG5Kamph5jWGNXdnIMe+3qnUmJ0RTXtdLQ1sk/1h3ihkfXs6u0gZ2l9RhjqG/twBUmNHd0s6ukARg8EZTYJYKSOmsw2/FQa3dtrdaeTaPOmvwq3s+vCnYY6jhxMhEUAVP83k8GShz8vHFvut8I4VOnJdPe1cMnH1jD95/f4dt+pKaFlo5uOrsNWXZbw8ZDtYA1VqGxLfD6ySV1VgN0j8E30vmTD7zPii3FTl1On1KLGl1+/e99/Or1vcEOQx0nTiaCDcBMEZkqIm7gGuBFBz9v3Pv04sm+17ecM40lORPIr2jirsvmsv2HF+MKEw7XtPjaB6alWGMUqps7fK932qWDFVuK+dbTW3zVRsV1rcS4XYC1TsLB6mY2Ha5jTb5zM6TW23FWDbFnkzp+Gto6aWrXrsShwrFEYIzpAm4DVgG7gWeMMTtF5BYRuQVARDJEpAj4b+AuESkSkXinYhrr0uKjuHapVciakhTDIzcs4aHPLeaLZ04lLiqCSYnRHKpuoc6ucjltem9j8+ULJwKwvaie7UX1fOOpLTy/uZifv7oHsKqEcnOsaTAOVTezw04YgzVID5UxJuB0GN6qIS0R9GWMYW/ZwD28joeG1i6a2jQRhApHxxEYY1YaY2YZY6YbY35qb3vQGPOg/brMGDPZGBNvjEm0Xzc4GdNY939XnsD2H15MVISLhJgILp6f4WtUzk6O4UhNi2808QmTErjrsrmANcndpMRothXXs6bAqvu9/pQsXt9VztYjdVQ2tbNoSiLJHjdbjtSxw25PKKodvPH41n9s4mtPbqare+B2hbtX7OD6R9b53tc2d7C/vNFXItDRz329u7+KS+5717GZYzccrPnQBK8lgtCiI4vHGBEhLioi4L4pSTEcqmnh+c3FREe4mJUex5fOmsamuy/ipCmJLJgUz47ietYdqGFaqof/vmgWAE/nHcEYmDwhmqVTk1hXWONLBCV1bfT09H6bP1zd4nuAA6zeX8lLW0u4e8XOgN/6O7p6WLG5hHUHamjtsHoH/fL1vXziD+/jPVwbi/vyNtyXDjKX1Edx/SMf8PSGw3zl7xv5/Vv7Bzyuq7uHlo5umtq7jnmtjJ4eQ4Nfu1RFQxs/fHEnnYN8cVDHnyaCcSQ7KYa6lk6e21TM9adk+bqkJtl/nzg5kQNVzby1p4JTpiaTHBtJQnQE79qzmuakeFg6NYniulbWFFQTFRFGR3cPlU29dfjXPLTWV51U39JJQ1sX2ckxPLn+sG9SPH9rC6tpbO+iu8ews8RKLvvLm2ju6O0yOtTRz6HCiZJSd49hTUE16wprqG7u8I1FCaTRrhLqMdB6jF17X9leymn/96avk8Lb+yp5fM3Bo6q+9pRpRUAwaSIYRxZMSgBgaoqHm8+ZNuB+gHNmWd1wc5JjKLLHI2QnxXDK1N52hf86yzqHt3qosa2Tkvo2thXVAXDIHnz2veVzyc2ewPOb+/YwOljVzO/f2o873PpntuWI9XMHq/sOWhvOfEhjWX5FE4+9f3Sy7M+bCOpGcObY+tZOjIGCqmaMGbwU1ujXNnCs7QQHq5pp7uj2lW4aAiS5jYdqWXbfarYXDdy92SmHqpvJr9DR7ZoIxpHTpyez+e6LeOvb55AWF3XU/tOmJXPredP50+cWc8n8dACykq3eRNERLlLjIpmTEcelJ2TwwPUnc8VJVgNzUW0r33tuOw+vth5i+8ubWL2/kv/ssUoS2ckxLM6ewP7yJl+Rv7m9i8/+eR27Shq4+7K5TEyIYltRPS0dXVT4fRv1uF2U1LdxqNq5Ec01zR2+Ko62zm7O+PlbvLj1+PdkfmLdYX700q4+VSWB9JYIBj9uOLwP3gL7oTdYacM/vsZjbCfwnss7LUqDnVj8P9/7RaOk/tg7JgzX3St2csez24775442mgjGERFhgsd91IhkL3d4GLdfModL/BqYc5J7J7UTEcLChAeuX8ylJ2QyMTEagNX7q3hy/WEefKcAgI7uHm54dD2/eWMfYLVNzM2Mp6O7hwNV1gP91//eR3FdK3+5aSmfOy2Hk7ISWXegmsJ+cxllJXvYXdrAOfe+PWiDs5cxJmDSGGh08pGaFk79vzd9D/595Y0U17Wy8WDNh37WQDq7e+js7uGx9w/wuzcHrms/Khb7gTfYZIHQmwhqR7BE4D2XtwF4sJ5aDX5tQM3HmgharZ+vaOxbIqjyq270lgj9256Ol/L6Nl9so92KLcW+waEjTRNBiMvqN7upvxh3OHMy4nh2UxFAnxHH/m2IsZHhzMm05jDaXdpAWX0bf1t7iM8snuLrkvqxEydS3tDOX9ce7PMZKbG9U2sU17VijKG9a+B66R+/vItz7n2bjYdq+cGKHTS2dbKusJoTfriKjYeOfri/sLmYju4ePii09nn/Ix0JsJZDaX2rr0F7MN98agvfeGozz28u5on1Q5+t9UhNi/3Zg/fEanAgEfQvATS2dw34e24Ywaoh78PdWwr0lhD84/G+bjhOiWBPWYOvOqi6uZ26ESx5OcUYw7ef2crL25wpyWoiCHE59kCzbLuKqL8vnJHT56GfEB1BhEtIiO7bc2laSiwRLmHDwRp+/PJOeozhtvNn+PZfOC+dlNhInsmzkop3AZ6fXLGAm86YCsCBqmaeXH+EpT990/ctraWji7f3VgDWg/Sx9w8C8PNXd/OXtYd44O0Cfvn6Xjq7Df/aWNxn5LQxhuftkdFb7faJXaVWIjhc0/dh3NbZzfL7V/Otp7d86O9sa1EdW4/UU1zbSml9m+9b86vbS/nxS7sCJhNjTG8iqBk8EfSWCI5+QFU1tXPdwx9wsGp4VWm1AaqCagd4ADpZNeRtf/BvF/K2V4xkieD1nWUDzrZ7x7Pb+dFLO+nuMdQ0d9DY3jXqezHVt3bS1WNIjo105PyaCELc9NRYIsPDmD8x8Di+K06axKTEaC6cmwbYDdFnT+Mnn1hAfFQ4Vy6y5hF0h4cxMy2Ov39wmJXby7jt/Bm+tRQAIlxhfPPCmQDERYVzctYEUmLd5KR4+Op50wHsHk3l1Ld28qd3CgG44c/rufGxDRysauY/dkIA2G9/o/v7B4fYcLCWCTERPLupiIU/et23tvPu0kYKK5uZmBDF3vJG2jp75106UtPSp2vkv3eVU9fSyWs7y1jjN8fOtqK6Pg/dru4eSuvbKKlv9T3AvNVh96zay6PvH+Dmv+Ud9Xusae7w9ZR6cv1hbnh0/YBVYYM1Fj+/qZg1BdW8uqOMru4efrlq75CqNgItWVo9QG+tkWws7i0R9K0aqu5TIrDiqAuQ+MBqcPbvwvxhvN+e//h2QcD9xbUtVDS0U9vSgfe0A312MD34TgG/eG0PJXWtVNmJ078EPZKcnIZajQFJHjfv33k+STGB/4FFRbh45/Zz6eoxLPjBKrKTY7j9kjkAXHZCJmF+zRH3fvpENh2uY9GUxD49lLw+e2o2Z85Iob2rh9S4SK5dmgVAssdNXGQ4hZXNbDhYS5hYD/ipKR7y7HmSdpVaxfnYyHCMMb7/uI1tXZw0JZFbzpnOLX/fCMBzm4o4Z1Yqb+wuRwS+dsFMvvfcdnYU17O7tIHoCBetnd1UNraTFm81qr+wuZj0+Ejc4WHc8dw2Xvn6WcS6w7np8TwSosO5/5pFvlJQd7+HUkFlE3My4nwzuK7eX0VFY1ufBnv/qqiCymYKKps5XNPCNL/5o7wG6z66YqtVwtl4qJalUyfw+//kkxLr5ka7VDWQQCWCgRqM/atojnVQma9E0Ni/sbg3CdUMUiI4UtPC+b96m/uuWcTH7dHxH6aupZPG9q6Ajc8dXT32Q1X6lErqWjpIjXPm2/ZHUdXU7uumHREmnD4jBYAULREop6TERhIWFriBGSDcFUZUhIufXrmAL57Z+8BxhUmfhun5ExP43KnZAZOAV06Kh9kZcSR53MzNtEohIsLUVA+v7yqjvrWT7y6bw4QYN3e9sIMM+0G9204E09NifSWNnOQYLpybxu+uXcQl89N55sun8clFk3hzdwVtnd38e1c5i6YkcsEcqzTzl7WHaO7o5mK7x9RFv3mXt/aU097Vzer9VVx+4kTuu3oRJXVt/GzlHvaUNVLV1E5BZTOX/+49bnxsfcD6/V+8uodvPL2Fts4ebj7b6nL7fn4VVU3tvrET3qoo/2903kWACiqb+NnK3b5vvg2+EkHfB+OBqmZ2FDcQ43ax6XAtu0sbfds/jH8PJO8tGygRNLZ1+br8DpYIyurb+NbTWwatTvM2Fld5E0GAEsFgVUM7S+rpMbD5cO2An9Gftzt0md1l9ZHVhby2owzoLZnUtvQdSxGoGi6YyvwGE5bWt/mSliYCFXRXL8nixMmJjpw7O9lDuV2PfNkJmTx+0xK+fPY0Vn3zbGamxfoSwYzUWCZPsBLBKVOTeeTzS5iSZPV4Wjo1iSsWTaKpvYtfvLaH7cX1XDgvnbT4KOZlxvOS3XPIWxKpb+3kgf8UsLeskY7uHhZnT2Bx9gQ+e0oW/8w7wlMbrIbg3GxrBbmCyuaAfd1L6tt4ZVspANcsmUJCdATfenorF/zqHS777Xvc98Y+DtgP/chwl+/n8iuaKK5r5bqHP+BP7xZy6W9Xc7C6mR4DiTERNLV39WmgX73fqvL6/Ok51DR3sGqn9XDbXdrI5x9dz6bDtXR292CMYeX20n4Pug7fpIKT7N5gA43faGjrJMXjxu0Ko6m9i10lDeQF6GX1q9f38vzmYp7fXOwrDfnzH1V8VGNx09GNxdZYh76lrb1lVhXgzmH0lvF2Ry2tb6O7x/Cbf+/jd/ZIau8DtrvHcKCqd/xAbUsH97+xn0vvX83K7aUBz1vT3NHn91Bc18rf1h6krbObNQVVR8Xu3xjf3N7Fr1/fO+QSlnfcRZhAWUObr5dVskNVQ5oI1KiwaEoiAF86cyqTJ0QzJyOe7106l4SYCOZkxrPuQA0Vje3MSItl8gTrQZYVoKfTmTNSOHVaEo+9f5CM+CiuzrUm6btonlUKmJcZz0n2Z4H1ANhqP9xPnGyVZL5y7gzCwoS/rj3EtBQP//rK6bxw6xkAPLH+MGECES7BFSYstXtFeU1N8XCB3Z4yLzOejy2cyH1v7OfR9w+Qmz2BX356IZefmElqXCT5FU3c+9oeGlq7+MknFtDS0c3be+1R3nbjvX87wdqCaiYlRvtmoV2932rLWH+whnf2VfKDFTvJ/d83uOrBtXz1H5t8D7/a5g6qmtp905hPTfEQJn1LBOsKq309aRpaO4mLisAT6aKprYu7V+zgtic289C7Bdz+z62+n9lR0uD7hrqusIa39pTzzac2+x6ITR1dGGOVgprau3x/Ilzi67XU1d3jK/lsOVLHnLtf801vAlZ3X4D1B2q4/HereW//h6+R4C21tXf1sOVInbUmR2kDtc0dlDX0ftPeV96bCCoa2njkvUJ2lTZw1ws7jjonwD2v7eGqB9fy1p5yAP70TgF3r9jJN57azHUPr+MBvzaJx94/wOy7XvMlpVe2l/Lbt/J5ZsORgOc2xvDi1hJfZ4cyO7GenDXBLhG0EyYwYYAq3GOliUCNCp8/PYdtP7yYuy6fd9Q4iLmZcb4GzJl+VUOBury6woTfXH0SF8xJ44+fPdnXy8KbCM6alUJUhIv37jiPr18wk8KqZtYWVJHkcfu+KWckRPGnzy7mzBkp3GRXhZ04KYFkj5tD1S143OFMmRBDRnwUj9yYy6a7L+JvX1zKb65eiIjwg4/N59/fOpsnbz6Vez51IpkJUdS3dnLr+TM4bXoyv7/uZGamxfL6zjJe3FrCDadlc9XJkwkTeM9uqJ5q9+Z6cWsJxhh6egwfFFZz6rRkpqXGstgupfjbXmwN2POuP1Fa30ZHVw+n/OxNthXVk50cgytMSI2NJMnj9lWT/HLVXq5+6ANu+ftGX/tLXFQ4sVHhVDS2sfVIHWUNbdz/xn7+ubGIQ9XNdHb3kF/RyCdPnkRcVDgfFFZz0+N5vLClpPcbvv2AP8VecvXFLSUYg+/+1TR3+KpkvDWT7V09rC3onfp8X3mjb9+O4gbue2MfpfWtR7XT+Cvya495c7f10DbGmu7Ev8pln1+vomc3FdPY1sU5s1Kpae7gz+8d4M5nt/X5HO9Efd9+Zitd3T28tcfqvLBqp/UZ967aS3FdKxUNbfzoJWtF3mfyijjj52/xpN3N+Jm8IwHnb9pWVM/Xn9zMsxuLfPcuPEyYPzGe8vo2Kps6SPK4cQ1ShXsstLFYjQquMCF+gMn0rlw0ib+uOURZQxvzJsYTE+lCBOZkBO7plJkQzZ9vXNJn2/yJ8dzzqRM53/62PnlCDIuyEjEGVm4v49zZqX0S0Hlz0jjPblsACAsTvrtsNnc8u50Z6bHMSoujtbPbF/NZM3tXzkuIjvA1LEe7XfziUyfyn70VnDur95gpE2JYU1BNXFQ4XzprGtFuFzPT4nyJ4OMLJ1Ja38r/vrKbJI+blNhIals6fVOLX3HSRDYeqmXyhGiKaluZluKhsqmdO5bNYW5mPPe9sY+Cyia2FdX5qpc6u3s4e2YKS6YmUdvSwYaDtbR1dvPIe4VMSowmv6KJf24sYtPhWm46cyqr91fx7r4quuyHYbNv0sB9zJ8YT2e3Yf7EeJbmJPGC3wJGB6qaSY6N9FUDXX5CJjuK630llJOmJFJY2czLW0s52/6dTEmK8S2TurOknpK6VpJj3Ryoaub06Sm+30veoVpO+9lb/PTKBbR2dHPB3HTCw4T0+Cjc4WG8tLWEN3dXEB4mdPUY3+vI8DDe2F3ep1PE/oomUmIjaWjtZMuROlJiI7nxjBze2VfJT162HuStnd2cMSOFq06e7KuuqW3p5I3d5X0SzrL5Gby2s4z1B6r7LCH7t7UHqW3ppLiuldjIcPaUNbKrtIHIcBfTUz2+f3Pe+b7y7RlnS+vbSI+PYmJiNI3tXRyqbibZ41xjtiYCNeplJkSz5s7zqW3pIDk2komJ0ay98wIyEo6eRmMgIsJnlkzps+0Ev0bts2d++BKoVy/JIjcnicjwMF87xVCcPSvV98DzOnd2Kqt2lfHYjUt8vVUWTErwLUU6MTGaJ750Kp/84xp+/PIujIFpqR6WLcgA4JolWTS2dTF/Yjw3PraBSxZk8J2LZ/u+MS6cnMiagmrfA/TW86Zz+YkTfQ30bZ3d/OelXazYUkxbZw93XTaXH7y4k+8/v52uHsP1p2Sx+XCtb9K5lFg3VU0dvrYWb3vLnIx40s6yuud6H4yFVc2kxEby/CYrOSTGuPncqdn87yu7Abhobjp1LZ385o19vk4KU1M8vkTw5u4KXthSwuUnZtLVY/jMkinMyYjjkydP5n9f2cWagmqe3VjEpsN17Ctv5Jm8IpbmJHHb+TP42pObgd6lXPeWNzI3M55Tpibx+JqDuMPDiHG7aOnopqa5gzkZcXT19NDR0sPF89OZl9n3y8WKLSWs2FLC2oJqDte0cNbMFFbvr/JVA108L53Xd5Xz5XOm8X5+FRsO1rK9qJ6TpiRS1dTeJ1l89bzp3PPaXn65ai//2VvJozfmcv4cq6T6rt3+4x15X1rfSmZClO/f+M6SBt/YGydo1ZAaE8LCpM9gmuEkgYGkxEbyveVz+O21i/jCGTlD+pnpfo3Vx2L5CZlsvvsiFmX1VvF4x3KcMyuVWemxhIUJ/3flCSR73Jw4OYHHblxCbKT13c0dHsat51lVTZ9cNImrFk/uU20wMz2W7h7D0xuOMCs9ltsvmeNLAoAvMf3opV24woQzZ6bw4OcWk5EQxfIFGWQne7hkvpV0Tp2WxKcWT+bCuek8/PlcHr4h13eeaakeTpuezHt3nE/+T5cT4RIefreQC3/9Do/Ys9HGR4f36foZHx3Bj6+YT4QrjJ+8vItJidF9SlTeQWwvbytlSlI0y+ZncNfl85g3MZ4n/utULpybzqbDdQC8sMVKSOsP1nDrE5uYmuIhLS6Sz52W7TtfbvYE7lg2h1npsXR09fjaX8AaWe9to7hwbhppcZHER1m/40dvzGXPT5Zx4+k5PL+5mI6uHi6cm06ES9hWVM+cjDjuumwet18ym4WTE1mcM4F/5h1he3E9yxZkMDPNapNZnD2BT5w0keuWZjEvM57/2O1A3tX/dpbU+67HmwjK6tvITIz29Zqrb+3UEoFSTvnyOdOD9tn920I+tXgykRFhfHrxFN++eRPjefPb5w54jshwF7+++qSjts+wH0Kl9W1cMDfrqP3TUjzMTo9jb3kjJ05OIC4qgpOzJvDu7ef5Bll96axpXHdKFmEiREX09naalBjN419Ywv7yJiJcvd8lw11hZCXFsL+iiaykGF+X2fioCN94De/7yRNi+P11i/jpK7u596qF7Cq1Goi9YzzOmJHM1iP13Llsrq8rq9eCSfG8Ydf9e6u9Fk5JJD0uktsvmc2MtFhEhOc2FdHc3s0dy+cQ7Xbx/FfP4IUtxcxOj+OqB9cCVkJ8fZd1rtOnpyAizM6II+9QLYuzk4iKcHHt0iweX3MQsNqopqfGsqeskXNnp5GVHMOt51kj6JfkJPH23krCBJYvyKCmuYP/7K3k/DlpvmPOmpniG93+wYFqfvLyLv629hApsW7OmZXKM3lFvJ9fRWl9GxfPjyIzIdp33U71GAJNBEqNGgnREVx/SvaHHzgE01NjCQ8ToiNc3BIg2YkIT958Ks/kHeFkv1KJiODyy08x7sCPiHNnp3Hu7LSjtnuP/+TJk9hT2shrO8t862F89tQs/v7BYSZ4ettVXvumVRLwDv7674tmsbusge9eMocJnog+3W29Fky0qvSiIsJo6+zhhEkJrLB7dflbceuZuMPDfCUlT2T4Ub/fc2alcu9VJ1Lb0uFLdssWZJKZEO1r55mVHsvEhChK6tvISfEwLzOePWWNnD+n7/VfuzQLEavqKzvZ40vG/uNqzp2dxp/eLWR6qocdxQ3sKG7g04snc8fyOWw4UMMzeUW+1fxmpMWSntBbCli+IPOoaxwpmgiUGoeiIlys+d75JEQHfpiCNao8UJI4FvHR1iPl4wsn8rXzPZTUteKxq7N+9PEFXLs0K2DV2sXzrAGBS3ImDDh7rpf3wfqpkyfz5PrDnDotKeBx0e7A1+1vSlJMn6lQgD6DJsFKjhfMTeeFzcVkxEdxyYIMiupaOTkrsc9xSR43Xz23d36tZQsyKKtv47RpvWt8nDY9mRduPYPKxnb+6695zEqP5d5PLwToM8r8ndvPJcseH/Py184kLT4y4NTyI0WOdSm64y03N9fk5R09l4tSKvhK6lrZeKiWjw1xOoiPatXOMpbkJLG/vJHZGXEkDrN/vXdg4AmTBx4F76+xrZOy+jZmpscNO9ZA2ru6ue+N/Xzh9BxftVl3j+Ge1/bw6dzJzEgbmc/xJyIbjTG5Afc5mQhEZBlwP+ACHjHG/LzffrH3Xwq0ADcaYzYNdk5NBEopNXyDJQLHeg2JiAv4A7AcmAdcKyLz+h22HJhp/7kZ+KNT8SillArMye6jS4F8Y0yhMaYDeAq4ot8xVwB/NZYPgEQRca5FRCml1FGcTASTAP+JNYrsbcM9BhG5WUTyRCSvsrJyxANVSqlQ5mQiCNT0379BYijHYIx5yBiTa4zJTU398BGgSimlhs7JRFAE+I/pnwz0X3BzKMcopZRykJOJYAMwU0SmiogbuAZ4sd8xLwI3iOVUoN4YE3gycKWUUo5wbECZMaZLRG4DVmF1H33UGLNTRG6x9z8IrMTqOpqP1X30C07Fo5RSKjBHRxYbY1ZiPez9tz3o99oAtzoZg1JKqcGNuZHFIlIJHPqIP54CfPgSR2ODXsvopNcyOum1QLYxJmBvmzGXCI6FiOQNNLJurNFrGZ30WkYnvZbB6XoESikV4jQRKKVUiAu1RPBQsAMYQXoto5Ney+ik1zKIkGojUEopdbRQKxEopZTqRxOBUkqFuJBJBCKyTET2iki+iNwZ7HiGS0QOish2EdkiInn2tiQR+beI7Lf/nvBh5wkGEXlURCpEZIfftgFjF5Hv2fdpr4hcEpyoAxvgWn4oIsX2vdkiIpf67RuV1yIiU0TkPyKyW0R2isg37O1j7r4Mci1j8b5Eich6EdlqX8uP7O3O3hdjzLj/gzXFRQEwDXADW4F5wY5rmNdwEEjpt+0e4E779Z3AL4Id5wCxnw2cDOz4sNixFjHaCkQCU+375gr2NXzItfwQ+E6AY0fttQCZwMn26zhgnx3vmLsvg1zLWLwvAsTaryOAdcCpTt+XUCkRDGWRnLHoCuAv9uu/AJ8IXigDM8a8C9T02zxQ7FcATxlj2o0xB7DmoVp6POIcigGuZSCj9lqMMaXGXhbWGNMI7MZaC2TM3ZdBrmUgo/lajDGmyX4bYf8xOHxfQiURDGkBnFHOAK+LyEYRudnelm7s2Vrtv9OCFt3wDRT7WL1Xt4nINrvqyFtsHxPXIiI5wCKsb59j+r70uxYYg/dFRFwisgWoAP5tjHH8voRKIhjSAjij3BnGmJOx1nm+VUTODnZADhmL9+qPwHTgJKAU+JW9fdRfi4jEAs8C3zTGNAx2aIBto/1axuR9McZ0G2NOwlqfZamILBjk8BG5llBJBGN+ARxjTIn9dwXwPFbxr9y7xrP9d0XwIhy2gWIfc/fKGFNu/+ftAR6mt2g+qq9FRCKwHpz/MMY8Z28ek/cl0LWM1fviZYypA94GluHwfQmVRDCURXJGLRHxiEic9zVwMbAD6xo+bx/2eWBFcCL8SAaK/UXgGhGJFJGpwExgfRDiGzLvf1DblVj3BkbxtYiIAH8Gdhtjfu23a8zdl4GuZYzel1QRSbRfRwMXAntw+r4Eu5X8OLbGX4rVm6AA+H6w4xlm7NOwegZsBXZ64weSgTeB/fbfScGOdYD4n8QqmndifYP54mCxA9+379NeYHmw4x/CtfwN2A5ss/9jZo72awHOxKpC2AZssf9cOhbvyyDXMhbvy4nAZjvmHcD/2NsdvS86xYRSSoW4UKkaUkopNQBNBEopFeI0ESilVIjTRKCUUiFOE4FSSoU4TQQqZInIGvvvHBG5boTP/f8CfZZSo5F2H1UhT0TOxZql8vJh/IzLGNM9yP4mY0zsCISnlOO0RKBCloh4Z3n8OXCWPWf9t+xJv+4VkQ32hGVfto8/1573/gmsgUqIyAv2RIA7vZMBisjPgWj7fP/w/yyx3CsiO8RaX+Jqv3O/LSL/EpE9IvIPe8SsUo4LD3YASo0Cd+JXIrAf6PXGmCUiEgm8LyKv28cuBRYYa8pfgJuMMTX2dAAbRORZY8ydInKbsSYO6++TWJOgLQRS7J951963CJiPNVfM+8AZwHsjfbFK9aclAqWOdjFwgz0V8Dqs4f0z7X3r/ZIAwNdFZCvwAdbkXzMZ3JnAk8aaDK0ceAdY4nfuImNNkrYFyBmBa1HqQ2mJQKmjCfA1Y8yqPhuttoTmfu8vBE4zxrSIyNtA1BDOPZB2v9fd6P9PdZxoiUApaMRa4tBrFfAVe2pjRGSWPetrfwlArZ0E5mAtKejV6f35ft4FrrbbIVKxlr4cFTNfqtCl3ziUsmZ67LKreB4H7seqltlkN9hWEngZ0NeAW0RkG9bMjx/47XsI2CYim4wx1/ttfx44DWsmWQN81xhTZicSpYJCu48qpVSI06ohpZQKcZoIlFIqxGkiUEqpEKeJQCmlQpwmAqWUCnGaCJRSKsRpIlBKqRD3/wFM/NAkmrkX/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f1f6bc-f2ba-4b06-9109-7778966e1379"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a78f947-6f88-4871-8005-d5732cd8e2d9"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2hbZHT8cNyO",
        "outputId": "9211347e-c402-44fa-d7d5-a23d90b2dcdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 22  / Predicted tensor([0]) / Actual label tensor(1)\n",
            "Sample 101  / Predicted tensor([1]) / Actual label tensor(0)\n",
            "Sample 182  / Predicted tensor([0]) / Actual label tensor(1)\n",
            "Sample 213  / Predicted tensor([1]) / Actual label tensor(0)\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "sample_index = 0\n",
        "for x_batch, y_batch in validation_loader:\n",
        "    for x, y in zip(x_batch, y_batch):\n",
        "        model.eval()\n",
        "        x = x.unsqueeze(0)\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        z = model(x)\n",
        "        _, yhat = torch.max(z, 1)\n",
        "                \n",
        "        if yhat[0] != y:\n",
        "            print(\"Sample\", sample_index, \" / Predicted\", yhat.cpu(), \"/ Actual label\", y.cpu())\n",
        "            count += 1\n",
        "        \n",
        "        sample_index += 1\n",
        "        \n",
        "        if count == 4: break\n",
        "    if count == 4: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "715b8fe6-26bd-4bb9-b8da-1ca492528ee6"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46f20a3f-7d1b-4aea-9e74-e373ec30e1bb"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb64cce-3fe5-489d-bced-79c3e7a447cf"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\">  IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a379170-e56f-40f9-9f8f-e3227416419a"
      },
      "source": [
        "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}